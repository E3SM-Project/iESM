<!-- Beg of tools chapter -->
<chapter id="tools">
<title>Using the &clm; tools to create your own input datasets</title>
<para>
There are several tools provided with &clm; that allow you to create your own input 
datasets at resolutions you choose, or to interpolate initial conditions to a different
resolution, or used to compare &clm; history files between different cases. The tools are
all available in the <filename>models/lnd/clm/tools</filename> directory. Most of the tools
are &FORTRAN; stand-alone programs in their own directory, but there is also a suite of
&ncl;
scripts in the <filename>ncl_scripts</filename> directory, and some of the tools are scripts that
may also call the ESMF regridding program. Some of the &ncl; scripts are
very specialized and not meant for general use, and we won't document them here. They
still contain documentation in the script itself and the README file in the tools
directory. But, the list of generally important scripts and programs are:
<orderedlist>
<listitem>
<para> <emphasis>cprnc</emphasis> to compare &netcdf; files with a time axis.</para>
</listitem>
<listitem>
<para> <emphasis>mkmapgrids</emphasis> to create &scrip; grid data files from old &clm; format grid
files that can then be used to create new &clm; datasets.</para>
<para> <emphasis>mkmapdata</emphasis> to create &scrip; mapping data file from &scrip; grid files (uses &esmf;).</para>
</listitem>
<listitem>
<para> <emphasis>gen_domain</emphasis> to create a domain file for datm from a mapping file. The domain
file is then used by BOTH datm AND &clm; to define the grid and land-mask.</para>
</listitem>
<listitem>
<para> <emphasis>mksurfdata_map</emphasis> to create surface datasets from grid datasets.</para>
</listitem>
<listitem>
<para> <emphasis>interpinic</emphasis> to interpolate initial condition files.</para>
</listitem>
<listitem>
<para> <emphasis>ncl_scripts/getregional_datasets.pl</emphasis> script to extract a
region or a single-point from global input datasets. See the single-point chapter
for more information on this.</para>
</listitem>
<listitem>
<para> <emphasis>mkprocdata_map</emphasis> to interpolate output unstructured grids (such as the 
&cam; HOMME dy-core "ne" grids like ne30np4) into a 2D regular lat/long grid format that can be plotted easily.</para>
</listitem>
</orderedlist>
</para>

<para>
In the sections to come we will go into detailed description of how to use each of
these tools in turn. First, however we will discuss the common environment variables
and options that are used by all of the &FORTRAN; tools. Second, we go over the outline
of the entire file creation process for all input files needed by &clm; for a new 
resolution, then we turn to each tool. In the last section we will
discuss how to customize files for particular observational sites.
</para>

<sect1 id="tool_build">
<title>Common environment variables and options used in building the &FORTRAN;
tools</title>
<para>
The &FORTRAN; tools all have similar makefiles, and similar options for building.
All of the Makefiles use GNU Make extensions and thus require that you use GNU make
to use them. They also auto detect the type of platform you are on, using "uname -s"
and set the compiler, compiler flags and such accordingly. There are also environment
variables that can be set to set things that must be customized. All the tools use
&netcdf; and hence require the path to the &netcdf; libraries and include files.
On some platforms (such as Linux) multiple compilers can be used, and hence there
are env variables that can be set to change the &FORTRAN; and/or "C" compilers used.
The tools other than <command>cprnc</command> also allow finer control, by also 
allowing the user to add compiler flags they choose, for both &FORTRAN; and "C", as 
well as picking the compiler, linker and and add linker options. Finally the tools 
other than <command>cprnc</command> allow you to turn
optimization on (which is off by default  but on for the <command>mksurfdata_map</command> and
<command>interpinic</command>
programs) with the <envar>OPT</envar> flag so that the 
tool will run faster. To get even faster performance, the <command>interpinic</command>,
program allows you to also use the <envar>SMP</envar> to 
turn on multiple shared memory processors.
When <envar>SMP=TRUE</envar> you set the number of threads used by the program with
the <envar>OMP_NUM_THREADS</envar> environment variable.
</para>
<para>
Options used by all: <command>cprnc</command>, <command>interpinic</command>, and
<command>mksurfdata_map</command>
<simplelist>
<member><envar>LIB_NETCDF</envar> -- sets the location of the &netcdf; library.</member>
<member><envar>INC_NETCDF</envar> -- sets the location of the &netcdf; include files.</member>
<member><envar>USER_FC</envar> -- sets the name of the &FORTRAN; compiler.</member>
</simplelist>
Options used by: <command>interpinic</command>, <command>mkprocdata_map</command>,
<command>mkmapgrids</command>, and <command>mksurfdata_map</command>
<simplelist>
<member><envar>MOD_NETCDF</envar> -- sets the location of the &netcdf; &FORTRAN; module.</member>
<member><envar>USER_LINKER</envar> -- sets the name of the linker to use.</member>
<member><envar>USER_CPPDEFS</envar> -- adds any CPP defines to use.</member>
<member><envar>USER_CFLAGS</envar> -- add any "C" compiler flags to use.</member>
<member><envar>USER_FFLAGS</envar> -- add any &FORTRAN; compiler flags to use.</member>
<member><envar>USER_LDFLAGS</envar> -- add any linker flags to use.</member>
<member><envar>USER_CC</envar> -- sets the name of the "C" compiler to use.</member>
<member><envar>OPT</envar> -- set to TRUE to compile the code optimized (TRUE or FALSE)</member>
<member><envar>SMP</envar> -- set to TRUE to turn on shared memory parallelism (i.e.
&omp;) (TRUE or FALSE)</member>
<member><filename>Filepath</filename> -- list of directories to build source code from.</member>
<member><filename>Srcfiles</filename> -- list of source code filenames to build executable from.</member>
<member><filename>Makefile</filename> -- customized makefile options for this particular tool.</member>
<member><filename>Makefile.common</filename> -- General tool Makefile that should be the same between all tools.</member>
</simplelist>
Options used only by cprnc:
<simplelist>
<member><envar>EXEDIR</envar> -- sets the location where the executable will be built.</member>
<member><envar>VPATH</envar> -- colon delimited path list to find the source files.</member>
</simplelist>
More details on each environment variable.
<variablelist>
<varlistentry>
<term><envar>LIB_NETCDF</envar></term><listitem> 
<para>
This variable sets the path to the &netcdf; library file 
(<filename>libnetcdf.a</filename>). If not 
set it defaults to <filename>/usr/local/lib</filename>. In order to use the tools
you need to build the &netcdf; library and be able to link to it. In order to build
the model with a particular compiler you may have to compile the &netcdf; library with
the same compiler (or at least a compatible one).
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>INC_NETCDF</envar></term><listitem> 
<para>
This variable sets the path to the &netcdf; include directory (in order to find
the include file <filename>netcdf.inc</filename>).
if not set it defaults to <filename>/usr/local/include</filename>.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>MOD_NETCDF</envar></term><listitem> 
<para>
This variable sets the path to the &netcdf; module directory (in order to find
the &netcdf; &FORTRAN90; module file when &netcdf; is used with a &FORTRAN90;
<command>use statement</command>. When not set it defaults to the 
<envar>LIB_NETCDF</envar> value.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_FC</envar></term><listitem> 
<para>
This variable sets the command name to the &FORTRAN90; compiler to use when
compiling the tool. The default compiler to use depends on the platform. And
for example, on the AIX platform this variable is NOT used
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_LINKER</envar></term><listitem> 
<para>
This variable sets the command name to the linker to use when linking the object
files from the compiler together to build the executable. By default this is set to
the value of the &FORTRAN90; compiler used to compile the source code.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_CPPDEFS</envar></term><listitem> 
<para>
This variable adds additional optional values to define for the C preprocessor.
Normally, there is no reason to do this as there are very few CPP tokens in the CLM
tools. However, if you modify the tools there may be a reason to define new CPP
tokens.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_CC</envar></term><listitem> 
<para>
This variable sets the command name to the "C" compiler to use when
compiling the tool. The default compiler to use depends on the platform. And
for example, on the AIX platform this variable is NOT used
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_CFLAGS</envar></term><listitem> 
<para>
This variable adds additional compiler options for the "C" compiler to use
when compiling the tool. By default the compiler options are picked according
to the platform and compiler that will be used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_FFLAGS</envar></term><listitem> 
<para>
This variable adds additional compiler options for the &FORTRAN90; compiler to use
when compiling the tool. By default the compiler options are picked according
to the platform and compiler that will be used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>USER_LDFLAGS</envar></term><listitem> 
<para>
This variable adds additional options to the linker that will be used when linking
the object files into the executable. By default the linker options are picked according
to the platform and compiler that is used.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>SMP</envar></term><listitem> 
<para>
This variable flags if shared memory parallelism (using &omp;) should be used when 
compiling the tool.  It can be set to either <literal>TRUE</literal> or 
<literal>FALSE</literal>, by default it is set to <literal>FALSE</literal>, so 
shared memory parallelism is NOT used. When set to <literal>TRUE</literal> you can
set the number of threads by using the <envar>OMP_NUM_THREADS</envar> environment
variable. Normally, the most you would set this to would be to the number of on-node
CPU processors. Turning this on should make the tool run much faster.
</para>
<caution>
<para>
Note, that depending on the compiler answers may be different when <envar>SMP</envar>
is activated.
</para>
</caution>
</listitem>
</varlistentry>

<varlistentry>
<term><envar>OPT</envar></term><listitem> 
<para>
This variable flags if compiler optimization should be used when 
compiling the tool.  It can be set to either <literal>TRUE</literal> or 
<literal>FALSE</literal>, by default it is set to <literal>FALSE</literal> for
<command>mkmapgrids</command> and <literal>TRUE</literal> for
<command>mksurfdata_map</command>, <command>mkprocdata_map</command> and 
<command>interpinic</command>.  Turning this on should make the tool run much faster.
</para>
<caution>
<para>
Note, you should expect that answers will be different when <envar>OPT</envar>
is activated.
</para>
</caution>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>Filepath</filename></term><listitem> 
<para>
All of the tools are stand-alone and don't  need any outside code to operate. The
<filename>Filepath</filename> is the list of directories needed to compile
and hence is always simply "." the current directory. Several tools use
copies of code outside their directory that is in the &cesm;
distribution (either csm_share code or &clm; source code).
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>Srcfiles</filename></term><listitem> 
<para>
The <filename>Srcfiles</filename> lists the filenames of the source code to use
when building the tool.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>Makefile</filename></term><listitem> 
<para>
The <filename>Makefile</filename> is the custom GNU Makefile for this particular tool. It
will customize the <envar>EXENAME</envar> and the optimization settings for this particular tool.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>Makefile.common</filename></term><listitem> 
<para>
The <filename>Makefile.common</filename> is the copy of the general GNU Makefile for all the &clm; tools.
This file should be identical between the different tools. This file has different sections of compiler
options for different Operating Systems and compilers.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>EXEDIR</filename></term><listitem> 
<para>
The <command>cprnc</command> tool uses this variable to set the location of where the executable
will be built. The default is the current directory.
</para>
</listitem>
</varlistentry>

<varlistentry>
<term><filename>VPATH</filename></term><listitem> 
<para>
The <command>cprnc</command> tool uses this variable to set the colon delimited pathnames of where
the source code exists.  The default is the current directory.
</para>
</listitem>
</varlistentry>

</variablelist>
</para>

<note>
<para>
There are several files that are copies of the original files from either
<filename>models/lnd/clm/src/main</filename>, 
<filename>models/csm_share/shr</filename>, or copies from other tool
directories. By having copies the tools can all be made stand-alone, but
any changes to the originals will have to be put into the tool directories
as well.
</para>
</note>

<para>The <emphasis>README.filecopies</emphasis> (which can be found in
<filename>models/lnd/clm/tools)</filename> is repeated here.</para>
<programlisting width="99">
&filecopies;
</programlisting>

</sect1>

<sect1 id="tool_run">
<title>General information on running the &FORTRAN; tools</title>
<para>
The tools run either one of two ways, with a namelist to provide options, or
with command line arguments (and NOT both). <command>interpinic</command>,
<command>gen_domain</command>  and <command>cprnc</command> run with command line 
arguments, and the other tools run with namelists.
</para>
<sect2 id="tool_run_namelist">
<title>Running &FORTRAN; tools with namelists</title>
<para>
<command>mksurfdata_map</command> and 
<command>mkmapgrids</command> run with namelists that are read from
standard input. Hence, you create a namelist and then run them by
redirecting the namelist file into standard input as follows:
<screen width="99">
./program &lt; namelist
</screen>
For programs with namelists there is at least one sample namelist with the
name "program".namelist (i.e. <filename>mksurfdata_map.namelist</filename> 
for the <command>mksurfdata_map</command> program). There may also be other sample
namelists that end in a different name besides "namelist". Namelists that you create 
should be similar to the example namelist. The namelist values are also documented 
along with the other namelists in the:
<ulink url="../../bld/namelist_files/namelist_definition.xml">
<filename>models/lnd/clm/bld/namelist_files/namelist_definition.xml</filename></ulink>
file and default values in the:
<ulink url="../../bld/namelist_files/namelist_defaults_clm_tools.xml">
<filename>models/lnd/clm/bld/namelist_files/namelist_defaults_clm_tools.xml</filename></ulink>
file.
</para>
</sect2>
<sect2 id="tool_run_commandline">
<title>Running &FORTRAN; tools with command line options</title>
<para>
<command>interpinic</command>, <command>gen_domain</command>, and <command>cprnc</command> run with command line
arguments. The detailed sections below will give you more information on the command
line arguments specific to each tool. Also running the tool without any arguments
will give you a general synopsis on how to run the tool. For example to get help
on running <command>interpinic</command> do the following.
<screen width="99">
cd models/lnd/clm/tools/interpinic
gmake
./interpinic
</screen>
</para>
</sect2>
<sect2 id="tool_run_smp">
<title>Running &FORTRAN; tools built with SMP=TRUE</title>
<para>
When you enable <envar>SMP=TRUE</envar> on your build of one of the tools that
make use of it, you are using &omp; for shared memory parallelism (SMP). In
SMP loops are run in parallel with different threads run on different processors
all of which access the same memory (called on-node). Thus you can only usefully 
run up to the number of processors that are available on a single-node of the machine
you are running on. For example, on the &ncar; machine yellowstone there are 16 processors
per node, but the SMT hardware on the machine allows you to submit twice as many 
threads or 32 threads. So to run <command>interpinic</command> on yellowstone 
optimized, with 32 threads you would do the following:
<screen width="99">
cd models/lnd/clm/tools/interpinic/src
gmake OPT=TRUE SMP=TRUE
setenv OMP_NUM_THREADS 32
cd ..
./interpinic `cat interpinic.runoptions`
</screen>
</para>
</sect2>
</sect1>

<sect1 id="ncl_scripts">
<title>Using &ncl; scripts</title>
<para>
In the tools directory <filename>models/lnd/clm/tools/ncl_scripts</filename> and in a few
other locations there are scripts that use &ncar; Command Language (&ncl;).
Unlike the &FORTRAN; tools, you will need to get a copy of &ncl; in order to use them. You also won't have to
build an executable in order to use them, hence no Makefile is provided. &ncl; is provided
for free download as either binaries or source code from:
<ulink url="http://www.ncl.ucar.edu/">http://www.ncl.ucar.edu/</ulink>. The &ncl;
web-site also contains documentation on &ncl; and it's use. These scripts are stand-alone and
at most use environment variables to control how to use them. In some cases there are perl scripts
with command line arguments that call the &ncl; scripts to control what they do.
</para>
</sect1>

<sect1 id="file_creation_process">
<title>The File Creation Process</title>

<para>
When just creating a replacement file for an existing one, the relevant tool should
be used directly to create the file. When you are creating a set of files for a new
resolution there are some dependencies between the tools that you need to keep in mind
when creating them. The main dependency is that you MUST create a &scrip; grid file first as the &scrip; 
grid dataset is then input into the other tools.  Also look at
<xref linkend="table_required_files"></xref> which gives information on the files required and when. 
<xref linkend="mkmapdata_mksurfdata"></xref> shows
an overview of the general data-flow for creation of the <filename>fsurdat</filename> datasets. 
<figure id="mkmapdata_mksurfdata">
<title>Data Flow for Creation of Surface Datasets from Raw &scrip; Grid Files</title>
<mediaobject>
<imageobject><imagedata fileref="mkmapdata_mksurfdata.jpeg" format="JPEG"/></imageobject>
<caption>
<para>
Starting from a &scrip; grid file that describes the grid you will run the model on, you first
run <command>mkmapdata.sh</command> to create a list of mapping files. The mapping files tell
<command>mksurfdata_map</command> how to map between the output grid and the raw datasets that
it uses as input. The output of <command>mksurfdata_map</command> is a surface dataset that you then
use for running the model. See <xref linkend="mksurfdata_details"></xref> for a more detailed view
of how <command>mksurfdata_map</command> works.
</para>
</caption>
</mediaobject>
</figure>
<xref linkend="CLMToolLegend"></xref> is the legend for this figure (<xref linkend="mkmapdata_mksurfdata"></xref>)
and other figures in this chapter (<xref linkend="GlobalDomainFig"></xref>, <xref linkend="mknoocnmapFig"></xref>, 
and <xref linkend="mksurfdata_details"></xref>).
<figure id="CLMToolLegend">
<title>Legend for Data Flow Figures</title>
<mediaobject>
<imageobject><imagedata fileref="LegendCLMToolDataFlow.jpeg" format="JPEG"/></imageobject>
<caption>
<para>
Green arrows define the input to a program, while red arrows define the output. Cylinders define files that are 
either created by a program or used as input for a program. Boxes are programs.
</para>
</caption>
</mediaobject>
</figure>
You start with a description of a &scrip; grid file for your output grid file and then create mapping files
from the raw datasets to it. Once, the mapping files are created <command>mksurfdata_map</command> is run
to create the surface dataset to run the model.
</para>

<procedure>
<title>Creating a Complete Set of Files for Input to &clm;</title>
<step performance="optional">
<title>Create &scrip; grid datasets (if NOT already done)</title>
<para>
First you need to create a desciptor file for your grid, that includes
the locations of cell centers and cell corners. There is also a "mask"
field, but in this case the mask is set to one everywhere (i.e. all of
the masks for the output model grid are "nomask"). An example &scrip; grid
file is: <filename>$CSMDATA/lnd/clm2/mappingdata/grids/SCRIPgrid_10x15_nomask_c110308.nc</filename>.
The <command>mkmapgrids</command> and <filename>mkscripgrid.ncl</filename> &ncl; script
in the <filename>models/lnd/clm/tools/mkmapgrids</filename> directory can help
you with this. &scrip; grid files for all the standard &clm; grids are already
created for you.
See <xref linkend="mkmapgrids"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Create domain dataset (if NOT already done)</title>
<para>
Next use <command>gen_domain</command> to create a domain file for use by
&datm; and &clm;. This is required, unless
a domain file was already created.
See <xref linkend="gen_domain"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Create mapping files for <command>mksurfdata_map</command> (if NOT already done)</title>
<para>
Create mapping files for <command>mksurfdata_map</command> with <command>mkmapdata.sh</command> in 
<filename>models/lnd/clm/tools/mkmapdata</filename>.  
See <xref linkend="mkmapdata"></xref> for more information on this.
</para>
</step>

<step>
<title>Create surface datasets</title>
<para>
Next use <command>mksurfdata_map</command> to create a surface dataset, using the mapping
datasets created on the previous step as input.
See <xref linkend="mksurfdata_map"></xref> for more information on this.
</para>
</step>

<step>
<title>Create some sort of initial condition dataset</title>

<para>
You then need to do one of the following three options to have an initial dataset
to start from.
</para>

<substeps>

<step performance="optional">
<title>Use spinup-procedures to create initial condition datasets</title>
<para>
The first option is to do the spinup procedures from arbitrary initial conditions
to get good initial datasets. This is the most robust method to use.
See <xref linkend="CLMSP_SPINUP"></xref>, <xref linkend="CN_SPINUP"></xref>, or 
<xref linkend="CNDV_SPINUP"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Use <command>interpinic</command> to interpolate existing initial
condition datasets</title>
<para>
The next option is to interpolate from spunup datasets at a different resolution, using
<command>interpinic</command>.
See <xref linkend="interpinic"></xref> for more information on this.
</para>
</step>

<step performance="optional">
<title>Start up from arbitrary initial conditions</title>
<para>
The last alternative is to run from arbitrary initial conditions without using any
spun-up datasets. This is inappropriate when using &clmcn; (bgc=cn or cndv) as it 
takes a long time to spinup Carbon pools. 
<warning>
<para>
This is NOT recommended as many fields in &clm; take a long time to equilibrate.
</para>
</warning>
</para>
</step>

</substeps>

</step>

<step performance="optional">
<title>Enter the new datasets into the &buildnml; XML database</title>
<para>
The last optional thing to do is to enter the new datasets into the &buildnml;
XML database. See <xref linkend="adding_files"></xref> for more information on
doing this. This is optional because the user may enter these files into their
namelists manually. The advantage of entering them into the database is so that
they automatically come up when you create new cases.
</para>
</step>

</procedure>

<para>
The <ulink url="../../tools/README"><filename>models/lnd/clm/tools/README</filename></ulink>
goes through the complete process for creating input files needed to run &clm;. We repeat
that file here:
<screen width="99">
&tools_readme;
</screen>
</para>

</sect1>

<sect1 id="cprnc">
<title>Using the <command>cprnc</command> tool to compare two history files</title>
<para>
<command>cprnc</command> is a tool shared by both &cam; and &clm; to compare two
&netcdf; history files.
It differences every field that has a time-axis that is also shared on both files,
and reports a summary of the difference. The summary includes the three largest 
differences, as well as the root mean square (RMS) difference. It also gives some
summary information on the field as well. You have to enter at least one file, and up to
two files. With one file it gives you summary information on the file, and with two it
gives you information on the differences between the two. At the end it will give you a
summary of the fields compared and how many fields were different and how many were
identical.
</para>
<para>
Options:
<simplelist>
<member>-m = do NOT align time-stamps before comparing</member>
<member>-v = verbose output</member>
<member>-ipr</member>
<member>-jpr</member>
<member>-kpr</member>
</simplelist>
See the <command>cprnc</command>
<ulink url="../../tools/cprnc/README">README</ulink> file for more details. 
<!--
which is
repeated here:
<screen width="99">
&cprnc_readme;
</screen>
-->
<note>
<para>
To compare files with OUT a time axis you can use the <command>cprnc.ncl</command>
&ncl; script in <filename>models/lnd/clm/tools/ncl_scripts</filename>. It won't give
you the details on the differences but will report if the files are identical or 
different.
</para>
</note>
</para>
</sect1>

<sect1 id="interpinic">
<title>Using <command>interpinic</command> to interpolate initial conditions to different
resolutions</title>
<para>
"interpinic" is used to interpolate initial conditions from one resolution to another.
In order to do the interpolation you must first run &clm; to create a restart file to
use as the "template" to interpolate into. Running from arbitrary initial conditions
(i.e. finidat = ' ') for a single time-step is sufficient to do this. Make sure the
model produces a restart file. You also need to make sure that you setup the same
configuration that you want to run the model with, when you create the template file.
</para>
<para>
Command line options to <command>interpinic</command>:
<simplelist>
<member>-i = Input filename to interpolate from</member>
<member>-o = Output interpolated file, and starting template file</member>
</simplelist>
</para>
<para>
There is a sample template file in the <filename>models/lnd/clm/tools/interpinic</filename>
directory and can be used to run interpolate to.
However, this file was created with an older version of &clm; and hence
we actually recommend that you would do a short run with &clm; to create a template file
to use.
</para>
<para>

<example id="example_createtemplate">
<title>Example of running &clm; to create a template file for
<command>interpinic</command> to interpolate to</title>
<screen width="99">
> cd scripts
> ./create_newcase -case cr_f10_TmpltI1850CN -res f10_f10 -compset I1850CN \
-mach yellowstone_intel
> cd cr_f10_TmpltI1850CN
# Set starting date to end of year, align to starting year, run a cold start, for one day
> ./xmlchange RUN_STARTDATE=1948-12-31,DATM_CLMNCEP_YR_ALIGN=1948,CLM_FORCE_COLDSTART=on,STOP_N=1
# Then setup, build and run as normal
> ./cesm_setup -case
> ./cr_f10_TmpltI1850CN.build
> ./cr_f10_TmpltI1850CN.submit
# And copy the resulting restart file to your interpinic directory
> cd ../models/lnd/clm/tools/interpinic
> cp /ptmp/$LOGIN/cr_f10_TmpltI1850CN/run/cr_f10_TmpltI1850CN.clm2.r.1949-01-01-00000.nc .
</screen>
</example>
</para>
<para>
In the next example we build <command>interpinic</command> optimized with shared
memory on for 64 threads so that it runs as fast as possible, to interpolate one of
the standard 1-degree datasets to the above 10x15 template file that we created.
</para>
<example id="example_interpinic">
<title>Example of building and running <command>interpinic</command> to 
interpolate a 1-degree <filename>finidat</filename> dataset to 10x15</title>
<screen width="99">
> cd models/lnd/clm/tools/interpinic
> gmake OPT=TRUE SMP=TRUE
> env OMP_NUM_THREADS=64 ./interpinic -o cr_f10_TmpltI1850CN.clm2.r.1949-01-01-00000.nc /
-i /fs/cgd/csm/inputdata/ccsm4_init/b40.1850.track1.1deg.006/0863-01-01/b40.1850.track1.1deg.006.clm2.r.0863-01-01-00000.nc
</screen>
<para>
<tip>
<para>
Running <command>interpinic</command> at high resolution can take a long time, so we
recommend that you always build it optimized and with shared memory processing on, to
cut down the run time as much as possible.
</para>
</tip>
<warning>
<para>
<command>interpinic</command> does NOT work for CNDV (bgc=cndv).
</para>
</warning>
</para>
</example>

</sect1>

<sect1 id="mkmapgrids">
<title>Creating an output &scrip; grid file at a resolution to run the model on </title>
<para> <emphasis>mkmapgrids</emphasis> to create &scrip; grid data files on a grid to run the model on.
The program converts old formats of &cam; or &clm; grid files to &scrip; grid format. There is also a &ncl;
script (<filename>mkscripgrid.ncl</filename>) to create regular latitude longitude regional or single-point grids at 
the resolution the user desires.
</para>
<para>
&scrip; grid files for all the standard model resolutions and the raw surface datasets have already been done and
the files are in the XML database. Hence, this step doesn't need to be done -- EXCEPT WHEN YOU ARE CREATING YOUR
OWN GRIDS. If you have a &clm; grid or &cam; file from previous versions and you want to convert it you can use 
<command>mkmapgrids</command>.
</para>

<sect2 id="mknoocnmap">

<title>Using <command>mknocnmap.pl</command> to create grid and maps for single-point regional grids</title>
<para>
If you want to create a regular latitude/longitude single-point or regional grid, we
suggest you use <command>mknoocnmap.pl</command> in <filename>models/lnd/clm/tools/mkmapdata</filename> which will
create both the &scrip; grid file you need (using <filename>models/lnd/clm/tools/mkmapgrids/mkscripgrid.ncl</filename>
AND an identity mapping file assuming there is NO ocean in your grid domain. If you HAVE ocean in your domain you could
modify the mask in the &scrip; grid file for ocean, and then use <command>ESMF_RegridWeightGen</command> to create
the mapping file, and <command>gen_domain</command> to create the domain file. Like other tools,
<filename>mkmapdata/mknoocnmap.pl</filename> has a help option with the following:
<screen width="99">
&mknoocnmappl;
</screen>
See <xref linkend="mknoocnmapFig"></xref> for a visual representation of this process.
</para>
</sect2>

</sect1>

<sect1 id="mkmapdata">
<title>Creating mapping files that <command>mksurfdata_map</command> will use</title>
<para> <emphasis>mkmapdata</emphasis> to create &scrip; mapping data file from &scrip; grid files (uses &esmf;).
The bash shell script <filename>models/lnd/clm/tools/mkmapgrids/mkmapdata.sh</filename> uses &esmfregrid; to
create a list of maps from the raw datasets that are input to <command>mksurfdata_map</command>. Each dataset that
has a different grid, or land-mask needs a different mapping file for it, but many different raw datasets share
the same grid/land-mask as other files. Hence, there doesn't need to be a different mapping file for EACH raw
dataset -- just for each DIFFERENT raw dataset. The bash script figures out which mapping files it needs to create
and then runs &esmfregrid; for each one. You can then either enter the datasets into the XML database (see
 <xref linkend="adding_files"></xref> or leave the files in place, and use the "-res usrspec -usr_gname -usr_gdate"
options to <command>mksurfdata_map</command> (see <xref linkend="mksurfdata.pl"></xref> below). 
<filename>mkmapdata.sh</filename> has a help option with the following:
<screen width="99">
&mkmapdatash;
</screen>
</para>
</sect1>
<sect1 id="gen_domain">
<title>Creating a domain file for &clm; and &datm;</title>
<para> <emphasis>gen_domain</emphasis> to create a domain file for datm from a mapping file. The domai
file is then used by BOTH datm AND &clm; to define the grid and land-mask. The general data flow is shown
in two figures. <xref linkend="GlobalDomainFig"></xref> shows the general flow for a general global case (or for a
regional grid that DOES include ocean). <xref linkend="mknoocnmapFig"></xref> shows the use of
<command>mknoocnmap.pl</command> (see <xref linkend="mknoocnmap"></xref>) to create a regional or single-point map file that is then run through
<command>gen_domain</command> to create the domain file for it.  As stated before <xref linkend="CLMToolLegend"></xref> 
is the legend for both of these figures. See the <ulink url="../../../../../mapping/gen_domain_files/README">
<filename>mapping/gen_domain_files/README</filename></ulink> file for more help on <command>gen_domain</command>.
</para>
<para>
Here we create domain files for a regular global domain.
<figure id="GlobalDomainFig">
<title>Global Domain file creation</title>
<mediaobject>
<imageobject><imagedata fileref="GlobalDomain.jpeg" format="JPEG"/></imageobject>
<caption>
<para>
Starting from &scrip; grid files for both your atmosphere and ocean, you use <command>gen_cesm_maps.sh</command> to
create a mapping file between the atmosphere and ocean. That mapping file is then used as input to
<command>gen_domain</command> to create output domain files for both atmosphere and ocean. The atmosphere domain
file is then used by both &clm; and &datm; for I compsets, while the ocean domain file is ignored. For this
process you have to define your &scrip; grid files on your own. For a regional or single-point case that doesn't
include ocean see <xref linkend="mknoocnmapFig"></xref>.
(See <xref linkend="CLMToolLegend"></xref> for the legend for this figure)
</para>
</caption>
</mediaobject>
</figure>
Note, that the &scrip; grid file used to start this process, is also used in <command>mkmapdata.sh</command> (see 
<xref linkend="mkmapdata"></xref>). Next we create domain files for a single-point or regional domain.
<figure id="mknoocnmapFig">
<title>Domain file creation using mknoocnmap.pl</title>
<mediaobject>
<imageobject><imagedata fileref="mknoocnmap.jpeg" format="JPEG"/></imageobject>
<caption>
<para>
For a regular latitude/longitude grid that can be used for regional or single point simulations -- you
can use <command>mknoocnmap.pl</command>. It creates a &scrip; grid file that can then be used as input
to <command>mkmapdata.sh</command> as well as a &scrip; mapping file that is then input to <command>gen_domain</command>.
The output of <command>gen_domain</command> is a atmosphere domain file used by both &clm; and &datm; and
a ocean domain file that is ignored.
(See <xref linkend="CLMToolLegend"></xref> for the legend for this figure)
</para>
</caption>
</mediaobject>
</figure>
In this case the process creates both &scrip; grid files to be used by <command>mkmapdata.sh</command>
as well as the domain files that will be used by both &clm; and &datm;.
</para>

</sect1>

<sect1 id="getregional_datasets.pl">
<title>Creating a set of regional datasets from existing global datasets</title>
<para>
Use the <emphasis>ncl_scripts/getregional_datasets.pl</emphasis> script to extract a
region or a single-point from global input datasets. See <xref linkend="getregional_datasets.pl"></xref>
in the single-point chapter for more information on this.
</para>
</sect1>

<sect1 id="mksurfdata_map">
<title>Using mksurfdata_map to create surface datasets from grid datasets</title>
<para>
<command>mksurfdata_map</command> is used to create surface-datasets from grid datasets and raw datafiles
at half-degree resolution to produce files that describe the surface characteristics
needed by &clm; (fraction of grid cell covered by different land-unit types, and fraction
for different vegetation types, as well as things like soil color, and soil texture,
etc.). To run <command>mksurfdata_map</command> you can either use the
<command>mksurfdata.pl</command> script which will create namelists for you using the &buildnml;
XML database, or you can run it by hand using a namelist that you provide (possibly
modeled after an example provided in the
<filename>models/lnd/clm/tools/mksurfdata_map</filename> directory). The namelist for 
<command>mksurfdata_map</command> is sufficiently complex that we recommend using the 
<command>mksurfdata.pl</command> tool to build them. It also requires that mapping files
from your output grid to the raw datasets that <command>mksurfdata_map</command> are made
to regrid relevant datasets (see <xref linkend="mkmapdata"></xref> and <xref linkend="mkmapdata_mksurfdata"></xref>
for a visual representation of the process). For standard resolutions these mapping files are
already created, but if you want to run for your own single-point or region, you'll need
to create these mapping files. In the next section 
we describe how to use the <command>mksurfdata.pl</command> script and the following
section gives more details on running <command>mksurfdata_map</command> by hand and the
various namelist input variables to it.
</para>
<sect2 id="mksurfdata.pl">
<title>Running <command>mksurfdata.pl</command></title>
<para>
The script <command>mksurfdata.pl</command> can be used to run the 
<command>mksurfdata_map</command> program for several configurations, resolutions, 
simulation-years and simulation year ranges. It will create the needed namelists for 
you and move the files
over to your inputdata directory location (and create a list of the files created, and
for developers this file is also a script to import the files into the svn inputdata
repository). It will also use the &buildnml; XML database
to determine the correct input files to use, and for transient cases it will create
the appropriate <filename>mksrf_fdynuse</filename> file with the list of files for each
year needed for this case. And in the case of urban single-point 
datasets (where surface datasets are actually input into <command>mksurfdata_map</command>)
it will do the additional processing required so that the output dataset
can be used once again by <command>mksurfdata_map</command>. Because, it figures out
namelist and input files for you, it is recommended that you use this script for creation
of standard surface datasets. If you need to create surface datasets for customized
cases, you might need to run <command>mksurfdata_map</command> on it's own. But you
could use <command>mksurfdata.pl</command>with the "-debug" option to give you
a namelist to start from. The list of files needed is very long and not necessisarily 
easy to figure out.
For help on <command>mksurfdata.pl</command> you can use the "-help" option as below:
<screen width="99">
> cd models/lnd/clm/tools/mksurfdata_map
> ./mksurdata.pl -help
</screen>
The output of the above command is:
<screen width="99">
&mksurfdatapl;
</screen>
</para>
<para>
To run the script with optimized <command>mksurfdata_map</command> for a 4x5 degree grid 
for 1850 conditions, on yellowstone you would do the following:
<example id="example_mksurfdata.pl">
<title>Example of running <command>mksurfdata.pl</command> to create a 4x5 resolution
<filename>fsurdat</filename> for a 1850 simulation year</title>
<screen width="99">
> cd models/lnd/clm/tools/mksurfdata_map
> gmake USER_FC=ifort
> ./mksurfdata.pl -y 1850 -r 4x5
</screen>
</example>
</para>
</sect2>

<sect2 id="mksurfdata_map_byhand">
<title>Running <command>mksurfdata_map</command> by Hand</title>
<para>
In the above section we show how to run <command>mksurfdata_map</command> through
the <command>mksurfdata.pl</command> using input datasets that are in the &buildnml;
XML database. When you are running with input datasets that are NOT available in
the XML database you either need to add them as outlined in 
<xref linkend="adding_files"></xref>, or you need to run <command>mksurfdata_map</command> 
by hand, as we will outline here. The easiest way to start is to use the "-debug" option
to <command>mksurfdata.pl</command> for a case as close as possible and then customize
the resulting <filename>namelist</filename> file for the datasets that you change.
</para>

<sect3 id="mksurfdata_map_namelist">
<title>Preparing your <command>mksurfdata_map</command> namelist</title>
<para>
When running <command>mksurfdata_map</command> by hand you will need to prepare your
own input namelist. There is a sample namelist setup for running on the
previous &ncar; machine bluefire. So you will need to change the filepaths to
use that namelist. The sample namelist is called
<simplelist>
<member><filename>mksurfdata_map.namelist</filename> -- standard sample namelist.</member>
<member><filename>pftdyn_hist_simyr1850-2005.txt</filename> -- the <varname>mksrf_fdynuse</varname> 
text file with filenames for 1850-2005.</member>
</simplelist>
Note, that one of the inputs <varname>mksrf_fdynuse</varname> is a filename that
includes the filepaths to other files. The filepaths in this file will have to
be changed as well. You also need to make sure that the line lengths remain the same
as the read is a formatted read, so the placement of the year in the file, must remain
the same, even with the new filenames. One advantage of the <command>mksurfdata.pl</command>
script is that it will create the <varname>mksrf_fdynuse</varname> file for you.
</para>
<para>
We list the namelist items below. Most of the namelist items are filepaths to give to
the input half degree resolution datasets that you will use to scale from to the
resolution of your grid dataset.
You must first specify the input grid dataset for the resolution to output for:
<orderedlist>
<listitem><para><varname>mksrf_fgrid</varname> mapping file that defines the output grid to run on</para></listitem>
</orderedlist>
Then you must specify settings for input high resolution datafiles
<orderedlist>
<listitem><para><varname>mksrf_ffrac</varname> land fraction and land mask dataset</para></listitem>
<listitem><para><varname>mksrf_fglacier</varname> Glacier dataset</para></listitem>
<listitem><para><varname>mksrf_flai</varname> Leaf Area Index dataset</para></listitem>
<listitem><para><varname>mksrf_flanwat</varname> Land water dataset</para></listitem>
<listitem><para><varname>mksrf_forganic</varname> Organic soil carbon dataset</para></listitem>
<listitem><para><varname>mksrf_fmax</varname> Max fractional saturated area dataset</para></listitem>
<listitem><para><varname>mksrf_fsoicol</varname> Soil color dataset</para></listitem>
<listitem><para><varname>mksrf_fsoitex</varname> Soil texture dataset</para></listitem>
<listitem><para><varname>mksrf_ftopo</varname> Topography dataset (this is used to limit
the extent of urban regions and is used for glacier multiple elevation classes)
</para></listitem>
<listitem><para><varname>mksrf_furban</varname> Urban dataset</para></listitem>
<listitem><para><varname>mksrf_fvegtyp</varname> PFT vegetation type dataset</para></listitem>
<listitem><para><varname>mksrf_fvocef</varname> Volatile Organic Compound Emission Factor
dataset</para></listitem>
</orderedlist>
Then the list of mapping files for each of these datasets. The same mapping file can be used by multiple
raw datasets (from the list above) if they are on the same grid and land-mask. Each mapping file needs to
correspond to the grid and land-mask of the raw datasets above.
<orderedlist>
 <listitem><para><varname>map_fglacier</varname> mapping file for <varname>mksrf_fglacier</varname></para></listitem>
 <listitem><para><varname>map_flai</varname> mapping file for <varname>mksrf_flai</varname></para></listitem>
 <listitem><para><varname>map_flakwat</varname> mapping file for <varname>mksrf_flakwat</varname></para></listitem>
 <listitem><para><varname>map_forganic</varname> mapping file for <varname>mksrf_forganic</varname></para></listitem>
 <listitem><para><varname>map_fmax</varname> mapping file for <varname>mksrf_fmax</varname></para></listitem>
 <listitem><para><varname>map_fsoicol</varname> mapping file for <varname>mksrf_fsoicol</varname></para></listitem>
 <listitem><para><varname>map_fsoitex</varname> mapping file for <varname>mksrf_fsoitex</varname></para></listitem>
 <listitem><para><varname>map_furbtopo</varname> mapping file for <varname>mksrf_furbtopo</varname></para></listitem>
 <listitem><para><varname>map_flndtopo</varname> mapping file for <varname>mksrf_flndtopo</varname></para></listitem>
 <listitem><para><varname>map_fharvest</varname> mapping file for <varname>mksrf_fharvest</varname></para></listitem>
 <listitem><para><varname>map_fwetlnd</varname> mapping file for <varname>mksrf_fwetlnd</varname></para></listitem>
 <listitem><para><varname>map_furban</varname> mapping file for <varname>mksrf_furban</varname></para></listitem>
 <listitem><para><varname>map_fpft</varname> mapping file for <varname>mksrf_fpft</varname></para></listitem>
 <listitem><para><varname>map_fvocef</varname> mapping file for <varname>mksrf_fvocef</varname></para></listitem>
</orderedlist>
<note>
<para>
If you add new raw datasets to <command>mksurfdata_map</command>, you will need to add the corresponding mapping file
for that dataset as well. If the file is on the same grid and land-mask as another dataset, it can share the same
mapping file. If it is on a different grid and/or land-mask -- YOU WILL NEED TO CREATE MAPPING DATASETS FOR IT. And
<command>mkmapdata.sh</command> will need to be changed to create the new mapping files (see <xref linkend="mkmapdata"></xref>.
See  <xref linkend="mksurfdata_details"></xref> for a visual representation of the relationship of the various
input and output files for <command>mksurfdata_map</command>.
<figure id="mksurfdata_details">
<title>Details of running <command>mksurfdata_map</command></title>
<mediaobject>
<imageobject><imagedata fileref="mksurfdata_details.jpeg" format="JPEG"/></imageobject>
<caption>
<para>
Each of the raw datasets (the <envar>mksrf_*</envar> files) needs a mapping file to map from the output grid
you are running on to the grid and land-mask for that dataset. Some raw datasets share the same grid and land
mask -- hence they can share the same mapping file.  One of the mapping files is used to specify the grid 
for <envar>mksrf_fgrid</envar>.
</para>
</caption>
</mediaobject>
</figure>
</para>
</note>
You specify the ASCII text file with the land-use files.
<orderedlist>
<listitem><para><varname>mksrf_fdynuse</varname> "dynamic land use" for transient
land-use/land-cover changes. This is an ASCII text file that lists the filepaths
to files for each year and then the year it represents (note: you MUST change the
filepaths inside the file when running on a machine NOT at &ncar;).
We always use this file, even for creating datasets of a fixed year. Also note
that when using the "pft_" settings this file will be an XML-like file with settings
for PFT's rather than filepaths (see <xref linkend="mksurfdata_map_exp"></xref> below).
</para>
</listitem>
</orderedlist>
And optionally you can specify settings for:
<orderedlist>
<listitem><para><varname>all_urban</varname> If entire area is urban (typically used for
single-point urban datasets, that you want to be exclusively urban)</para></listitem>
<listitem><para><varname>mksrf_firrig</varname> Irrigation dataset, if you want
activate the irrigation model over generic cropland
(experimental mode, normally NOT used). If this dataset is set, you also NEED to set
the mapping file for the irrigation dataset with <varname>map_firrig</varname>.</para></listitem>
<listitem><para><varname>mksrf_gridnm</varname> Name of output grid resolution (if not
set the files will be named according to the number of longitudes by latitudes)</para></listitem>
<listitem><para><varname>mksrf_gridtype</varname> Type of grid (default is 'global')</para></listitem>
<listitem><para><varname>nglcec</varname> number of glacier multiple elevation classes.
Can be 0, 1, 3, 5, or 10. When using the resulting dataset with &clm; you can then run
with <varname>glc_nec</varname> of either 0 or this value.
 (experimental normally use the default of 0, when running with the land-ice
model in practice only 10 has been used)</para></listitem>
<listitem><para><varname>numpft</varname> number of Plant Function Types (PFT) 
in the input vegetation <varname>mksrf_fvegtyp</varname> dataset. You change
this to 20, if you want to create a dataset with prognostic crop activated. The
vegetation dataset also needs to have prognostic crop types on it as well.
 (experimental normally not changed from the default of 16)</para></listitem>
<listitem><para><varname>outnc_large_files</varname> If output should be in &netcdf; large file
format</para></listitem>
<listitem><para><varname>outnc_double</varname> If output should be in double
precision (normally we turn this on)</para></listitem>
<listitem><para><varname>pft_frc</varname> array of fractions to override PFT
data with for all gridpoints (experimental mode, normally NOT used).</para></listitem>
<listitem><para><varname>pft_idx</varname> array of PFT indices to override PFT
data with for all gridpoints (experimental mode, normally NOT used).</para></listitem>
<listitem><para><varname>soil_clay</varname> percent clay soil to override
all gridpoints with (experimental mode, normally NOT used).</para></listitem>
<listitem><para><varname>soil_color</varname> Soil color to override
all gridpoints with (experimental mode, normally NOT used).</para></listitem>
<listitem><para><varname>soil_fmax</varname> Soil maximum fraction to override
all gridpoints with (experimental mode, normally NOT used).</para></listitem>
<listitem><para><varname>soil_sand</varname> percent sandy soil to
override all gridpoints with (experimental mode, normally NOT used).</para></listitem>
</orderedlist>
</para>
<para>
After creating your namelist,
when running on a non &ncar; machine you will need to get the files
from the inputdata repository.
In order to retrieve the files needed for mksurfdata_map you can do the following on your
namelist to get the files from the inputdata repository, using the
<command>check_input_data</command> script which also allows you to export data to
your local disk.
<example id="getmksurfdata_map_datasets">
<title>Getting the raw datasets for <command>mksurfdata_map</command> to your local 
machine using the <command>check_input_data</command> script</title>
<screen width="99">
> cd models/lnd/clm/tools/mksurfdata_map
# First remove any quotes and copy into a filename that can be read by the
# check_input_data script
> sed "s/'//g" namelist > clm.input_data_list
# Run the script with -export and give the location of your inputdata with $CSMDATA
> ../../../../../scripts/ccsm_utils/Tools/check_input_data -datalistdir . \
-inputdata $CSMDATA -check -export
# You must then do the same with the fdynuse file referred to in the namelist
# in this case we add a file = to the beginning of each line
> awk '{print "file = "$1}' pftdyn_hist_simyr2000-2000.txt > clm.input_data_list
# Run the script with -export and give the location of your inputdata with $CSMDATA
> ../../../../../scripts/ccsm_utils/Tools/check_input_data -datalistdir . \
-inputdata $CSMDATA -check -export
</screen>
</example>
</para>
<sect4 id="mksurfdata_map_exp">
<title>Single Point options to <command>mksurfdata_map</command></title>
<para>
The options: pft_frc, pft_idx, soil_clay, soil_color, soil_fmax, and soil_sand exist
to override the values that come in on the datasets with user specified values.
They override the PFT and soil values for all grid points to the given values that you set. This is useful for
running with single-point tower sites where the soil type and vegetation is known.
Note that when you use pft_frc, all other landunits will be zeroed out, and the
sum of your pft_frc array MUST equal 100.0. Also note that when using the "pft_" 
options the <filename>mksrf_fdynuse</filename> file instead of having filepath's
will be an XML-like file with PFT settings. Unlike the file of file-paths, you will
have to create this file by hand, <command>mksurfdata.pl</command> will NOT be able
to create it for you (other than the first year which will be set to the values 
entered on the command line). 
<!--
Note, that when &ptclm; is run, it CAN create these
files for you from a simpler format (see <xref linkend="PTCLMDynPFTFiles"></xref>).
-->
Instead of a filepath you have a list of XML elements that give information on the PFT's
and harvesting for example:
<screen width="99">
&lt;pft_f&gt;100&lt;/pft_f&gt;&lt;pft_i&gt;1&lt;/pft_i&gt;&lt;harv&gt;0,0,0,0,0&lt;/harv&gt;&lt;graz&gt;0&lt;/graz&gt;
</screen>
So the &lt;pft_f&gt; tags give the PFT fractions and the &lt;pft_i&gt; tags give the
index for that fraction. Harvest is an array of five elements, and grazing is a single
value. Like the usual file each list of XML elements goes with a year, and there is 
limit on the number of characters that can be used.
</para>
</sect4>
</sect3>

<sect3 id="mksurfdata_map_sop">
<title>Standard Practices when using <command>mksurfdata_map</command></title>
<para>
In this section we give the recommendations for how to use <command>mksurfdata_map</command>
to give similar results to the files that we created when using it.
</para>
<para>
If you look at the standard surface datasets that we have created and provided for use,
there are three practices that we have consistently done in each (you also see these in
the sample namelists and in the <command>mksurfdata.pl</command> script). The first is 
that we always output data in double precision (hence <varname>outnc_double</varname> 
is set to <literal>.true.</literal>). The next is that we always use the procedure 
for creating transient datasets (using <varname>mksrf_fdynuse</varname>) even when 
creating datasets for a fixed simulation year. This is to ensure that the fixed year
datasets will be consistent with the transient datasets. When this is done a 
"surfdata.pftdyn" dataset will be created -- but will NOT be used in &clm;. If you look
at the sample namelist <filename>mksurfdata_map.namelist</filename> you note that it
sets <varname>mksrf_fdynuse</varname> to the file
<filename>pftdyn_hist_simyr2000.txt</filename>, where the single file entered is
the same PFT file used in the rest of the namelist (as <varname>mksrf_fvegtyp</varname>).
The last practice that we always do is to always set <varname>mksrf_ftopo</varname>,
even if glacier elevation classes are NOT active. This is
important in limiting urban areas based on topographic height, and hence is important
to use all the time. The glacier multiple elevation classes will be used as well if
you are running a compset with the active glacier model.
</para>
<para>
There are two other important practices for creating urban single point datasets. The
first is that you often will want to set <varname>all_urban</varname> to
<literal>.true.</literal> so that the dataset will have 100% of the gridcell output
as urban rather than some mix of: urban, vegetation types, and other landunits. The
next practice is that most of our specialized urban datasets have custom values for
the urban parameters, hence we do NOT want to use the global urban dataset to get
urban parameters -- we use a previous version of the surface dataset for the urban
parameters. However, in order to do this, we need to append onto the previous surface
dataset the grid and land mask/land fraction information from the grid and fraction
datasets. This is done in <command>mksurfdata.pl</command> using the <acronym>NCO</acronym>
program <command>ncks</command>. An example of doing this for the Mexico City, Mexico
urban surface dataset is as follows:
<screen width="99">
> ncks -A $CSMDATA/lnd/clm2/griddata/griddata_1x1pt_mexicocityMEX_c090715.nc \
$CSMDATA/lnd/clm2/surfdata/surfdata_1x1_mexicocityMEX_simyr2000_c100407.nc
> ncks -A $CSMDATA/lnd/clm2/griddata/fracdata_1x1pt_mexicocityMEX_navy_c090715.nc \
$CSMDATA/lnd/clm2/surfdata/surfdata_1x1_mexicocityMEX_simyr2000_c100407.nc
</screen>
Note, if you look at the current single point urban surface datasets you will note
that the above has already been done.
</para>
<para>
The final issue is how to build <command>mksurfdata_map</command>. When NOT optimized
<command>mksurfdata_map</command> is a bit slower, but not too bad. Previous versions
of <command>mksurfdata_map</command> were much slower because it calculated the
mapping between each grid on the fly. Now, the mapping is done outside <command>mksurfdata_map</command>
using the &esmf; regridding tools.  So you may want to run it optimized, which is the default.
The problem with running optimized is that
answers will be different when running optimized versus non-optimized for most
compilers. So if you want answers to be the same as a previous surface dataset, you
will need to run it on the same platform and optimization level.
Note, that the output surface datasets
will have attributes that describe whether the file was written out optimized or not,
to enable the user to more easily try to match datasets created previously. For more information on the different
compiler options for the &clm4; tools see <xref linkend="tool_build"></xref>.
</para>
</sect3>
</sect2>

</sect1>

<sect1 id="mkprocdata_map">
<title>Converting unstructured grid output to gridded datasets for post processing</title>
<para>
<emphasis>mkprocdata_map</emphasis> to interpolate output unstructured grids (such as the
&cam; HOMME dy-core "ne" grids like ne30np4) into a 2D regular lat/long grid format that can be plotted easily.
It does this by using the &esmf; regrid mapping utility to map between the model grid and a standard
latitude-longitude grid of your choosing.
See the <ulink url="../../tools/mkprocdata_map/README">README</ulink> file for more information
on how to run this program.
</para>
</sect1>

<sect1 id="customizing_files">
<title>How to Customize Datasets for particular Observational Sites</title>
<para>
There are two ways to customize datasets for a particular observational site. The first
is to customize the input to the tools that create the dataset, and the second is to
over-write the default data after you've created a given dataset. Depending on the tool
it might be easier to do it one way or the other. In <xref
linkend="table_required_files"></xref> we list the files that are most likely to be
customized and the way they might be customized. Of those files, the ones you are most
likely to customize are: fatmlndfrc, fsurdat, faerdep (for &datm;), and 
stream_fldfilename_ndep.  Note <command>mksurfdata_map</command> as documented previously
has options to overwrite the vegetation and soil types. For more information on this also see
<xref linkend="own_single_point_datasets"></xref>. 
<!--
And &ptclm; uses these methods to
customize datasets see <xref linkend="PTCLMDOC"></xref>.
-->
</para>
<para>
Another aspect of customizing your input datasets is customizing the input atmospheric
forcing datasets. See the <xref linkend="own_atm_forcing"></xref> for more
information on this. 
<!--
Also the chapter on &ptclm; in <xref linkend="AmeriFluxdata"></xref> 
has information on using the AmeriFlux tower site data as atmospheric forcing.
-->
</para>
</sect1>

<sect1 id="tools_conclude">
<title>Conclusion of tools description</title>
<para>
We've given a description of how to use the different tools with &clm; to create
customized datasets. In the next chapter we will talk about how to make these
files available for build-namelist so that you can easily create simulations
that include them. In the chapter on single-point and regional datasets we also
give an alternative way to enter new datasets without having to edit files.
</para>
</sect1>

</chapter>
<!-- End of tools chapter -->
